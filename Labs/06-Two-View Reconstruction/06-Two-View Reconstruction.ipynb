{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision, Lab 6: Two-View Reconstruction\n",
    "\n",
    "Today we'll take a look at how to perform 3D reconstruction of a scene using point correspondences between two calibrated views of that scene.\n",
    "\n",
    "We'll explore keypoint detection and matching, estimating the essential matrix, estimating the camera rotation and translation, and resolving the scale ambiguity using extrinsic camera parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keypoint detection and matching\n",
    "\n",
    "Download <link>[these sample frames for stereo reconstruction.](https://www.cs.ait.ac.th/~mdailey/class/vision/stereo-frames.zip)</link>\n",
    "\n",
    "Using the <link>[ORB/AKAZE OpenCV tutorial](https://docs.opencv.org/4.3.0/dc/d16/tutorial_akaze_tracking.html)</link> as a guide, get AKAZE and ORB keypoints from the first two frames in the sequence. Note that the tutorial has some things such as setting the ROI and tracking from a video that are not relevant. Focus on the keypoint detector setup and keypoint matcher setup. (Codes are below)\n",
    "\n",
    "### Planar tracking\n",
    "\n",
    "The algorithm is as follows:\n",
    "\n",
    "- Detect and describe keypoints on the first frame, manually set object boundaries\n",
    "- For every next frame:\n",
    "  1. Detect and describe keypoints\n",
    "  2. Match them using bruteforce matcher\n",
    "  3. Estimate homography transformation using RANSAC\n",
    "  4. Filter inliers from all the matches\n",
    "  5. Apply homography transformation to the bounding box to find the object\n",
    "  6. Draw bounding box and inliers, compute inlier ratio as evaluation metric\n",
    "  \n",
    "### AKAZE vs ORB?\n",
    "\n",
    "Reference: <link>[Comparing ORB and AKAZE for visual odometry\n",
    "of unmanned aerial vehicles](http://www.epacis.net/ccis2016/papers/paper_121.pdf)</link>\n",
    "\n",
    "In **ORB**, the detection step is based on the FAST keypoint detector,\n",
    "which is an efficient corner detector suitable for real-time applications due\n",
    "to its computation properties. Since FAST does not include an orientation\n",
    "operator, ORB adds an orientation component to it, which\n",
    "is called oFAST (oriented FAST).\n",
    "\n",
    "**AKAZE** makes use of Fast Explicit Diffusion (FED) scheme embedded in a pyramidal framework in order to build an accelerate feature detection in nonlinear scale spaces. By means of FED schemes, a nonlinear scale space can be built much faster than with any other kind of discretization scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the OpenCV <link>[<code>drawKeypoints()</code>](https://docs.opencv.org/4.3.0/d4/d5d/group__features2d__draw.html#ga5d2bafe8c1c45289bc3403a40fb88920)</link> function to display the keypoints detected in the two images. Your result should look like this:\n",
    "\n",
    "<img src=\"img/lab06-1.png\" width=\"600\"/>\n",
    "\n",
    "Next, get matches using the brute force Hamming matcher, remove indistinct matches (matches for which the ratio of distances for the first and second match is greater than 0.8) and use the OpenCV <link>[<code>drawMatches()</code>](https://docs.opencv.org/4.3.0/d4/d5d/group__features2d__draw.html#gad8f463ccaf0dc6f61083abd8717c261a)</link> function to display the result for AKAZE and ORB.\n",
    "\n",
    "In your report, discuss which keypoint detector seems to work best in terms of number of matches and number of accurate matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C++ / main.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <opencv2/opencv.hpp>\n",
    "#include <vector>\n",
    "#include <iostream>\n",
    "#include <iomanip>\n",
    "#include \"stats.h\" // Stats structure definition\n",
    "#include \"utils.h\" // Drawing and printing functions\n",
    "using namespace std;\n",
    "using namespace cv;\n",
    "const double akaze_thresh = 3e-4; // AKAZE detection threshold set to locate about 1000 keypoints\n",
    "const double ransac_thresh = 2.5f; // RANSAC inlier threshold\n",
    "const double nn_match_ratio = 0.8f; // Nearest-neighbour matching ratio\n",
    "const int bb_min_inliers = 100; // Minimal number of inliers to draw bounding box\n",
    "const int stats_update_period = 10; // On-screen statistics are updated every 10 frames\n",
    "namespace example {\n",
    "    class Tracker\n",
    "    {\n",
    "    public:\n",
    "        Tracker(Ptr<Feature2D> _detector, Ptr<DescriptorMatcher> _matcher) :\n",
    "            detector(_detector),\n",
    "            matcher(_matcher)\n",
    "        {}\n",
    "        void setFirstFrame(const Mat frame, vector<Point2f> bb, string title, Stats& stats);\n",
    "        Mat process(const Mat frame, Stats& stats);\n",
    "        Ptr<Feature2D> getDetector() {\n",
    "            return detector;\n",
    "        }\n",
    "    protected:\n",
    "        Ptr<Feature2D> detector;\n",
    "        Ptr<DescriptorMatcher> matcher;\n",
    "        Mat first_frame, first_desc;\n",
    "        vector<KeyPoint> first_kp;\n",
    "        vector<Point2f> object_bb;\n",
    "    };\n",
    "    void Tracker::setFirstFrame(const Mat frame, vector<Point2f> bb, string title, Stats& stats)\n",
    "    {\n",
    "        cv::Point* ptMask = new cv::Point[bb.size()];\n",
    "        const Point* ptContain = { &ptMask[0] };\n",
    "        int iSize = static_cast<int>(bb.size());\n",
    "        for (size_t i = 0; i < bb.size(); i++) {\n",
    "            ptMask[i].x = static_cast<int>(bb[i].x);\n",
    "            ptMask[i].y = static_cast<int>(bb[i].y);\n",
    "        }\n",
    "        first_frame = frame.clone();\n",
    "        cv::Mat matMask = cv::Mat::zeros(frame.size(), CV_8UC1);\n",
    "        cv::fillPoly(matMask, &ptContain, &iSize, 1, cv::Scalar::all(255));\n",
    "        detector->detectAndCompute(first_frame, matMask, first_kp, first_desc);\n",
    "\n",
    "        Mat res;\n",
    "        drawKeypoints(first_frame, first_kp, res, Scalar(255, 0, 0), DrawMatchesFlags::DRAW_RICH_KEYPOINTS);\n",
    "        imshow(\"key points\", res);\n",
    "        waitKey(0);\n",
    "        destroyWindow(\"key points\");\n",
    "\n",
    "        stats.keypoints = (int)first_kp.size();\n",
    "        drawBoundingBox(first_frame, bb);\n",
    "        putText(first_frame, title, Point(0, 60), FONT_HERSHEY_PLAIN, 5, Scalar::all(0), 4);\n",
    "        object_bb = bb;\n",
    "        delete[] ptMask;\n",
    "    }\n",
    "    Mat Tracker::process(const Mat frame, Stats& stats)\n",
    "    {\n",
    "        TickMeter tm;\n",
    "        vector<KeyPoint> kp;\n",
    "        Mat desc;\n",
    "        tm.start();\n",
    "        detector->detectAndCompute(frame, noArray(), kp, desc);\n",
    "        stats.keypoints = (int)kp.size();\n",
    "        vector< vector<DMatch> > matches;\n",
    "        vector<KeyPoint> matched1, matched2;\n",
    "        matcher->knnMatch(first_desc, desc, matches, 2);\n",
    "        for (unsigned i = 0; i < matches.size(); i++) {\n",
    "            if (matches[i][0].distance < nn_match_ratio * matches[i][1].distance) {\n",
    "                matched1.push_back(first_kp[matches[i][0].queryIdx]);\n",
    "                matched2.push_back(kp[matches[i][0].trainIdx]);\n",
    "            }\n",
    "        }\n",
    "        stats.matches = (int)matched1.size();\n",
    "        Mat inlier_mask, homography;\n",
    "        vector<KeyPoint> inliers1, inliers2;\n",
    "        vector<DMatch> inlier_matches;\n",
    "        if (matched1.size() >= 4) {\n",
    "            homography = findHomography(Points(matched1), Points(matched2),\n",
    "                RANSAC, ransac_thresh, inlier_mask);\n",
    "        }\n",
    "        tm.stop();\n",
    "        stats.fps = 1. / tm.getTimeSec();\n",
    "        if (matched1.size() < 4 || homography.empty()) {\n",
    "            Mat res;\n",
    "            hconcat(first_frame, frame, res);\n",
    "            stats.inliers = 0;\n",
    "            stats.ratio = 0;\n",
    "            return res;\n",
    "        }\n",
    "        for (unsigned i = 0; i < matched1.size(); i++) {\n",
    "            if (inlier_mask.at<uchar>(i)) {\n",
    "                int new_i = static_cast<int>(inliers1.size());\n",
    "                inliers1.push_back(matched1[i]);\n",
    "                inliers2.push_back(matched2[i]);\n",
    "                inlier_matches.push_back(DMatch(new_i, new_i, 0));\n",
    "            }\n",
    "        }\n",
    "        stats.inliers = (int)inliers1.size();\n",
    "        stats.ratio = stats.inliers * 1.0 / stats.matches;\n",
    "        vector<Point2f> new_bb;\n",
    "        perspectiveTransform(object_bb, new_bb, homography);\n",
    "        Mat frame_with_bb = frame.clone();\n",
    "        if (stats.inliers >= bb_min_inliers) {\n",
    "            drawBoundingBox(frame_with_bb, new_bb);\n",
    "        }\n",
    "        Mat res;\n",
    "        drawMatches(first_frame, inliers1, frame_with_bb, inliers2,\n",
    "            inlier_matches, res,\n",
    "            Scalar(255, 0, 0), Scalar(255, 0, 0));\n",
    "        return res;\n",
    "    }\n",
    "}\n",
    "int main(int argc, char** argv)\n",
    "{\n",
    "    string video_name = \"robot.mp4\";\n",
    "    VideoCapture video_in;\n",
    "    video_in.open(video_name);\n",
    "    if (!video_in.isOpened()) {\n",
    "        cerr << \"Couldn't open \" << video_name << endl;\n",
    "        return 1;\n",
    "    }\n",
    "    Stats stats, akaze_stats, orb_stats;\n",
    "    Ptr<AKAZE> akaze = AKAZE::create();\n",
    "    akaze->setThreshold(akaze_thresh);\n",
    "    Ptr<ORB> orb = ORB::create();\n",
    "    Ptr<DescriptorMatcher> matcher = DescriptorMatcher::create(\"BruteForce-Hamming\");\n",
    "    example::Tracker akaze_tracker(akaze, matcher);\n",
    "    example::Tracker orb_tracker(orb, matcher);\n",
    "    Mat frame;\n",
    "    namedWindow(video_name, WINDOW_NORMAL);\n",
    "    cout << \"\\nPress any key to stop the video and select a bounding box\" << endl;\n",
    "    while (waitKey(1) < 1)\n",
    "    {\n",
    "        video_in >> frame;\n",
    "        cv::resizeWindow(video_name, frame.size());\n",
    "        imshow(video_name, frame);\n",
    "    }\n",
    "    vector<Point2f> bb;\n",
    "    cv::Rect uBox = cv::selectROI(video_name, frame);\n",
    "    bb.push_back(cv::Point2f(static_cast<float>(uBox.x), static_cast<float>(uBox.y)));\n",
    "    bb.push_back(cv::Point2f(static_cast<float>(uBox.x + uBox.width), static_cast<float>(uBox.y)));\n",
    "    bb.push_back(cv::Point2f(static_cast<float>(uBox.x + uBox.width), static_cast<float>(uBox.y + uBox.height)));\n",
    "    bb.push_back(cv::Point2f(static_cast<float>(uBox.x), static_cast<float>(uBox.y + uBox.height)));\n",
    "    akaze_tracker.setFirstFrame(frame, bb, \"AKAZE\", stats);\n",
    "    orb_tracker.setFirstFrame(frame, bb, \"ORB\", stats);\n",
    "    Stats akaze_draw_stats, orb_draw_stats;\n",
    "    Mat akaze_res, orb_res, res_frame;\n",
    "    int i = 0;\n",
    "    for (;;) {\n",
    "        i++;\n",
    "        bool update_stats = (i % stats_update_period == 0);\n",
    "        video_in >> frame;\n",
    "        // stop the program if no more images\n",
    "        if (frame.empty()) break;\n",
    "        akaze_res = akaze_tracker.process(frame, stats);\n",
    "        akaze_stats += stats;\n",
    "        if (update_stats) {\n",
    "            akaze_draw_stats = stats;\n",
    "        }\n",
    "        orb->setMaxFeatures(stats.keypoints);\n",
    "        orb_res = orb_tracker.process(frame, stats);\n",
    "        orb_stats += stats;\n",
    "        if (update_stats) {\n",
    "            orb_draw_stats = stats;\n",
    "        }\n",
    "        drawStatistics(akaze_res, akaze_draw_stats);\n",
    "        drawStatistics(orb_res, orb_draw_stats);\n",
    "        vconcat(akaze_res, orb_res, res_frame);\n",
    "        cv::imshow(video_name, res_frame);\n",
    "        if (waitKey(1) == 27) break; //quit on ESC button\n",
    "    }\n",
    "    akaze_stats /= i - 1;\n",
    "    orb_stats /= i - 1;\n",
    "    printStatistics(\"AKAZE\", akaze_stats);\n",
    "    printStatistics(\"ORB\", orb_stats);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C++ / stats.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ifndef STATS_H\n",
    "#define STATS_H\n",
    "\n",
    "struct Stats\n",
    "{\n",
    "    int matches;\n",
    "    int inliers;\n",
    "    double ratio;\n",
    "    int keypoints;\n",
    "    double fps;\n",
    "\n",
    "    Stats() : matches(0),\n",
    "        inliers(0),\n",
    "        ratio(0),\n",
    "        keypoints(0),\n",
    "        fps(0.)\n",
    "    {}\n",
    "\n",
    "    Stats& operator+=(const Stats& op) {\n",
    "        matches += op.matches;\n",
    "        inliers += op.inliers;\n",
    "        ratio += op.ratio;\n",
    "        keypoints += op.keypoints;\n",
    "        fps += op.fps;\n",
    "        return *this;\n",
    "    }\n",
    "    Stats& operator/=(int num)\n",
    "    {\n",
    "        matches /= num;\n",
    "        inliers /= num;\n",
    "        ratio /= num;\n",
    "        keypoints /= num;\n",
    "        fps /= num;\n",
    "        return *this;\n",
    "    }\n",
    "};\n",
    "\n",
    "#endif // STATS_H#pragma once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C++ / utils.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ifndef UTILS_H\n",
    "#define UTILS_H\n",
    "\n",
    "#include <opencv2/opencv.hpp>\n",
    "#include <vector>\n",
    "#include \"stats.h\"\n",
    "\n",
    "using namespace std;\n",
    "using namespace cv;\n",
    "\n",
    "void drawBoundingBox(Mat image, vector<Point2f> bb);\n",
    "void drawStatistics(Mat image, const Stats& stats);\n",
    "void printStatistics(string name, Stats stats);\n",
    "vector<Point2f> Points(vector<KeyPoint> keypoints);\n",
    "Rect2d selectROI(const String& video_name, const Mat& frame);\n",
    "\n",
    "void drawBoundingBox(Mat image, vector<Point2f> bb)\n",
    "{\n",
    "    for (unsigned i = 0; i < bb.size() - 1; i++) {\n",
    "        line(image, bb[i], bb[i + 1], Scalar(0, 0, 255), 2);\n",
    "    }\n",
    "    line(image, bb[bb.size() - 1], bb[0], Scalar(0, 0, 255), 2);\n",
    "}\n",
    "\n",
    "void drawStatistics(Mat image, const Stats& stats)\n",
    "{\n",
    "    static const int font = FONT_HERSHEY_PLAIN;\n",
    "    stringstream str1, str2, str3, str4;\n",
    "\n",
    "    str1 << \"Matches: \" << stats.matches;\n",
    "    str2 << \"Inliers: \" << stats.inliers;\n",
    "    str3 << \"Inlier ratio: \" << setprecision(2) << stats.ratio;\n",
    "    str4 << \"FPS: \" << std::fixed << setprecision(2) << stats.fps;\n",
    "\n",
    "    putText(image, str1.str(), Point(0, image.rows - 120), font, 2, Scalar::all(255), 3);\n",
    "    putText(image, str2.str(), Point(0, image.rows - 90), font, 2, Scalar::all(255), 3);\n",
    "    putText(image, str3.str(), Point(0, image.rows - 60), font, 2, Scalar::all(255), 3);\n",
    "    putText(image, str4.str(), Point(0, image.rows - 30), font, 2, Scalar::all(255), 3);\n",
    "}\n",
    "\n",
    "void printStatistics(string name, Stats stats)\n",
    "{\n",
    "    cout << name << endl;\n",
    "    cout << \"----------\" << endl;\n",
    "\n",
    "    cout << \"Matches \" << stats.matches << endl;\n",
    "    cout << \"Inliers \" << stats.inliers << endl;\n",
    "    cout << \"Inlier ratio \" << setprecision(2) << stats.ratio << endl;\n",
    "    cout << \"Keypoints \" << stats.keypoints << endl;\n",
    "    cout << \"FPS \" << std::fixed << setprecision(2) << stats.fps << endl;\n",
    "    cout << endl;\n",
    "}\n",
    "\n",
    "vector<Point2f> Points(vector<KeyPoint> keypoints)\n",
    "{\n",
    "    vector<Point2f> res;\n",
    "    for (unsigned i = 0; i < keypoints.size(); i++) {\n",
    "        res.push_back(keypoints[i].pt);\n",
    "    }\n",
    "    return res;\n",
    "}\n",
    "#endif // UTILS_H#pragma once\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python / stats.py\n",
    "\n",
    "A small tips:\n",
    " - Use multilines comment (\"\"\" some data \"\"\") under *class name* or *function name* to make your intellisense in python class or function method.\n",
    " - use **:type** to define parameter type.\n",
    " - overload operator (+, -,* , /) can be created yourself. The function method is something like **__method__(self,...)**. Please try, it is very useful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Stats:\n",
    "    \"\"\"\n",
    "    Statistic class\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    matches=0 (int):\n",
    "        total number of matching\n",
    "\n",
    "    inliers=0 (int):\n",
    "        number of inliner matching\n",
    "\n",
    "    ratio=0. (float):\n",
    "        Nearest-neighbour matching ratio\n",
    "\n",
    "    keypoints=0 (int):\n",
    "        Wall\n",
    "\n",
    "    fps=0. (float):\n",
    "        frame per 1 sec\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    add(Stats) - overload + function:\n",
    "        plus the information into this class\n",
    "\n",
    "    divide(Stats) - overload + function:\n",
    "        divide the information into this class\n",
    "    \"\"\"\n",
    "    matches:int\n",
    "    inliers:int\n",
    "    ratio:float\n",
    "    keypoints:int\n",
    "    fps:float\n",
    "\n",
    "    def __init__(self, matches = 0, inliers = 0, ratio = 0., keypoints = 0, fps = 0.):\n",
    "        self.matches = matches\n",
    "        self.inliers = inliers\n",
    "        self.ratio = ratio\n",
    "        self.keypoints = keypoints\n",
    "        self.fps = fps\n",
    "\n",
    "    def __add__(self, op:\"Stats\") -> \"Stats\":\n",
    "        self.matches += op.matches\n",
    "        self.inliers += op.inliers\n",
    "        self.ratio += op.ratio\n",
    "        self.keypoints += op.keypoints\n",
    "        self.fps += op.fps\n",
    "        return self\n",
    "\n",
    "    def __truediv__(self, num:int) -> \"Stats\":\n",
    "        self.matches //= num\n",
    "        self.inliers //= num\n",
    "        self.ratio /= num\n",
    "        self.keypoints //= num\n",
    "        self.fps /= num\n",
    "        return self\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"matches({0}) inliner({1}) ratio({2:.2f}) keypoints({3}) fps({4:.2f})\".format(self.matches, self.inliers, self.ratio, self.keypoints, self.fps)\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "    def to_strings(self):\n",
    "        \"\"\"\n",
    "        Convert to string set of matches, inliners, ratio, and fps\n",
    "        \"\"\"\n",
    "        str1 = \"Matches: {0}\".format(self.matches)\n",
    "        str2 = \"Inliers: {0}\".format(self.inliers)\n",
    "        str3 = \"Inlier ratio: {0:.2f}\".format(self.ratio)\n",
    "        str4 = \"Keypoints: {0}\".format(self.keypoints)\n",
    "        str5 = \"FPS: {0:.2f}\".format(self.fps)\n",
    "        return str1, str2, str3, str4, str5\n",
    "\n",
    "    def copy(self):\n",
    "        return Stats(self.matches, self.inliers, self.ratio, self.keypoints, self.fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches(7) inliner(3) ratio(9.00) keypoints(12) fps(10.50)\n",
      "matches(2) inliner(1) ratio(3.00) keypoints(4) fps(3.50)\n"
     ]
    }
   ],
   "source": [
    "# test the class\n",
    "\n",
    "#from stats import Stats\n",
    "\n",
    "test1 = Stats(5, 2, 9, 4, 1.5)\n",
    "test2 = Stats(2, 1, 0, 8, 9)\n",
    "\n",
    "test1 + test2\n",
    "print(test1)\n",
    "test1 / 3\n",
    "print(test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python / Utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stats import Stats\n",
    "import cv2\n",
    "from typing import List #use it for :List[...]\n",
    "\n",
    "def drawBoundingBox(image, bb):\n",
    "    \"\"\"\n",
    "    Draw the bounding box from the points set\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        image (array):\n",
    "            image which you want to draw\n",
    "        bb (List):\n",
    "            points array set\n",
    "    \"\"\"\n",
    "    color = (0, 0, 255)\n",
    "    for i in range(len(bb) - 1):\n",
    "        b1 = (int(bb[i][0]), int(bb[i][1]))\n",
    "        b2 = (int(bb[i + 1][0]), int(bb[i + 1][1]))\n",
    "        cv2.line(image, b1, b2, color, 2)\n",
    "    b1 = (int(bb[len(bb) - 1][0]), int(bb[len(bb) - 1][1]))\n",
    "    b2 = (int(bb[0][0]), int(bb[0][1]))\n",
    "    cv2.line(image, b1, b2, color, 2)\n",
    "\n",
    "def drawStatistics(image, stat: Stats):\n",
    "    \"\"\"\n",
    "    Draw the statistic to images\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        image (array):\n",
    "            image which you want to draw\n",
    "        stat (Stats):\n",
    "            statistic values\n",
    "    \"\"\"\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "    str1, str2, str3, str4, str5 = stat.to_strings()\n",
    "\n",
    "    shape = image.shape\n",
    "\n",
    "    cv2.putText(image, str1, (0, shape[0] - 120), font, 2, (0, 0, 255), 3)\n",
    "    cv2.putText(image, str2, (0, shape[0] - 90), font, 2, (0, 0, 255), 3)\n",
    "    cv2.putText(image, str3, (0, shape[0] - 60), font, 2, (0, 0, 255), 3)\n",
    "    cv2.putText(image, str5, (0, shape[0] - 30), font, 2, (0, 0, 255), 3)\n",
    "\n",
    "def printStatistics(name: str, stat: Stats):\n",
    "    \"\"\"\n",
    "    Print the statistic\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        name (str):\n",
    "            image which you want to draw\n",
    "        stat (Stats):\n",
    "            statistic values\n",
    "    \"\"\"\n",
    "    print(name)\n",
    "    print(\"----------\")\n",
    "    str1, str2, str3, str4, str5 = stat.to_strings()\n",
    "    print(str1)\n",
    "    print(str2)\n",
    "    print(str3)\n",
    "    print(str4)\n",
    "    print(str5)\n",
    "    print()\n",
    "\n",
    "def Points(keypoints):\n",
    "    res = []\n",
    "    for i in keypoints:\n",
    "        res.append(i)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python / main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from stats import Stats\n",
    "from utils import drawBoundingBox, drawStatistics, printStatistics, Points\n",
    "\n",
    "akaze_thresh:float = 3e-4 # AKAZE detection threshold set to locate about 1000 keypoints\n",
    "ransac_thresh:float = 2.5 # RANSAC inlier threshold\n",
    "nn_match_ratio:float = 0.8 # Nearest-neighbour matching ratio\n",
    "bb_min_inliers:int = 100 # Minimal number of inliers to draw bounding box\n",
    "stats_update_period:int = 10 # On-screen statistics are updated every 10 frames\n",
    "\n",
    "class Tracker:\n",
    "    def __init__(self, detector, matcher):\n",
    "        self.detector = detector\n",
    "        self.matcher = matcher\n",
    "\n",
    "    def setFirstFrame(self, frame, bb, title:str):\n",
    "        iSize = len(bb)\n",
    "        stat = Stats()\n",
    "        ptContain = np.zeros((iSize, 2))\n",
    "        i = 0\n",
    "        for b in bb:\n",
    "            #ptMask[i] = (b[0], b[1])\n",
    "            ptContain[i, 0] = b[0]\n",
    "            ptContain[i, 1] = b[1]\n",
    "            i += 1\n",
    "        \n",
    "        self.first_frame = frame.copy()\n",
    "        matMask = np.zeros(frame.shape, dtype=np.uint8)\n",
    "        cv2.fillPoly(matMask, np.int32([ptContain]), (255,0,0))\n",
    "\n",
    "        # cannot use in ORB\n",
    "        # self.first_kp, self.first_desc = self.detector.detectAndCompute(self.first_frame, matMask)\n",
    "\n",
    "        # find the keypoints with ORB\n",
    "        kp = self.detector.detect(self.first_frame,None)\n",
    "        # compute the descriptors with ORB\n",
    "        self.first_kp, self.first_desc = self.detector.compute(self.first_frame, kp)\n",
    "\n",
    "        # print(self.first_kp[0].pt[0])\n",
    "        # print(self.first_kp[0].pt[1])\n",
    "        # print(self.first_kp[0].angle)\n",
    "        # print(self.first_kp[0].size)\n",
    "        res = cv2.drawKeypoints(self.first_frame, self.first_kp, None, color=(255,0,0), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "        \n",
    "        stat.keypoints = len(self.first_kp)\n",
    "        drawBoundingBox(self.first_frame, bb);\n",
    "\n",
    "        cv2.imshow(\"key points of {0}\".format(title), res)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyWindow(\"key points of {0}\".format(title))\n",
    "\n",
    "        cv2.putText(self.first_frame, title, (0, 60), cv2.FONT_HERSHEY_PLAIN, 5, (0,0,0), 4)\n",
    "        self.object_bb = bb\n",
    "        return stat\n",
    "\n",
    "    def process(self, frame):\n",
    "        stat = Stats()\n",
    "        start_time = time.time()\n",
    "        kp, desc = self.detector.detectAndCompute(frame, None)\n",
    "        stat.keypoints = len(kp)\n",
    "        matches = self.matcher.knnMatch(self.first_desc, desc, k=2)\n",
    "\n",
    "        matched1 = []\n",
    "        matched2 = []\n",
    "        matched1_keypoints = []\n",
    "        matched2_keypoints = []\n",
    "        good = []\n",
    "\n",
    "        for i,(m,n) in enumerate(matches):\n",
    "            if m.distance < nn_match_ratio * n.distance:\n",
    "                good.append(m)\n",
    "                matched1_keypoints.append(self.first_kp[matches[i][0].queryIdx])\n",
    "                matched2_keypoints.append(kp[matches[i][0].trainIdx])\n",
    "\n",
    "        matched1 = np.float32([ self.first_kp[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "        matched2 = np.float32([ kp[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "\n",
    "        stat.matches = len(matched1)\n",
    "        homography = None\n",
    "        if (len(matched1) >= 4):\n",
    "            homography, inlier_mask = cv2.findHomography(matched1, matched2, cv2.RANSAC, ransac_thresh)\n",
    "        dt = time.time() - start_time\n",
    "        stat.fps = 1. / dt\n",
    "        if (len(matched1) < 4 or homography is None):\n",
    "            res = cv2.hconcat([self.first_frame, frame])\n",
    "            stat.inliers = 0\n",
    "            stat.ratio = 0\n",
    "            return res, stat\n",
    "        inliers1 = []\n",
    "        inliers2 = []\n",
    "        inliers1_keypoints = []\n",
    "        inliers2_keypoints = []\n",
    "        for i in range(len(good)):\n",
    "            if (inlier_mask[i] > 0):\n",
    "                new_i = len(inliers1)\n",
    "                inliers1.append(matched1[i])\n",
    "                inliers2.append(matched2[i])\n",
    "                inliers1_keypoints.append(matched1_keypoints[i])\n",
    "                inliers2_keypoints.append(matched2_keypoints[i])\n",
    "        inlier_matches = [cv2.DMatch(_imgIdx=0, _queryIdx=idx, _trainIdx=idx,_distance=0) for idx in range(len(inliers1))]\n",
    "        inliers1 = np.array(inliers1, dtype=np.float32)\n",
    "        inliers2 = np.array(inliers2, dtype=np.float32)\n",
    "\n",
    "        stat.inliers = len(inliers1)\n",
    "        stat.ratio = stat.inliers * 1.0 / stat.matches\n",
    "        bb = np.array([self.object_bb], dtype=np.float32)\n",
    "        new_bb = cv2.perspectiveTransform(bb, homography)\n",
    "        frame_with_bb = frame.copy()\n",
    "        if (stat.inliers >= bb_min_inliers):\n",
    "            drawBoundingBox(frame_with_bb, new_bb[0])\n",
    "\n",
    "        res = cv2.drawMatches(self.first_frame, inliers1_keypoints, frame_with_bb, inliers2_keypoints, inlier_matches, None, matchColor=(255, 0, 0), singlePointColor=(255, 0, 0))\n",
    "        return res, stat\n",
    "\n",
    "    def getDetector(self):\n",
    "        return self.detector\n",
    "\n",
    "def main():\n",
    "    video_name = \"robot.mp4\"\n",
    "    video_in = cv2.VideoCapture()\n",
    "    video_in.open(video_name)\n",
    "    if (not video_in.isOpened()):\n",
    "        print(\"Couldn't open \", video_name)\n",
    "        return -1\n",
    "\n",
    "    akaze_stats = Stats()\n",
    "    orb_stats = Stats()\n",
    "\n",
    "    akaze = cv2.AKAZE_create()\n",
    "    akaze.setThreshold(akaze_thresh)\n",
    "\n",
    "    orb = cv2.ORB_create()\n",
    "\n",
    "    matcher = cv2.DescriptorMatcher_create(\"BruteForce-Hamming\")\n",
    "\n",
    "    akaze_tracker = Tracker(akaze, matcher)\n",
    "    orb_tracker = Tracker(orb, matcher)\n",
    "\n",
    "    cv2.namedWindow(video_name, cv2.WINDOW_NORMAL);\n",
    "    print(\"\\nPress any key to stop the video and select a bounding box\")\n",
    "\n",
    "    key = -1\n",
    "\n",
    "    while(key < 1):\n",
    "        _, frame = video_in.read()\n",
    "        w, h, ch = frame.shape\n",
    "        cv2.resizeWindow(video_name, (h, w))\n",
    "        cv2.imshow(video_name, frame)\n",
    "        key = cv2.waitKey(1)\n",
    "\n",
    "    print(\"Select a ROI and then press SPACE or ENTER button!\")\n",
    "    print(\"Cancel the selection process by pressing c button!\")\n",
    "    uBox = cv2.selectROI(video_name, frame);\n",
    "    bb = []\n",
    "    bb.append((uBox[0], uBox[1]))\n",
    "    bb.append((uBox[0] + uBox[2], uBox[0] ))\n",
    "    bb.append((uBox[0] + uBox[2], uBox[0] + uBox[3]))\n",
    "    bb.append((uBox[0], uBox[0] + uBox[3]))\n",
    "\n",
    "    stat_a = akaze_tracker.setFirstFrame(frame, bb, \"AKAZE\",);\n",
    "    stat_o = orb_tracker.setFirstFrame(frame, bb, \"ORB\");\n",
    "\n",
    "    akaze_draw_stats = stat_a.copy()\n",
    "    orb_draw_stats = stat_o.copy()\n",
    "\n",
    "    i = 0\n",
    "    video_in.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    while True:\n",
    "        i += 1\n",
    "        update_stats = (i % stats_update_period == 0)\n",
    "        _, frame = video_in.read()\n",
    "        if frame is None:\n",
    "            # End of video\n",
    "            break\n",
    "        akaze_res, stat = akaze_tracker.process(frame)\n",
    "        akaze_stats + stat\n",
    "        if (update_stats):\n",
    "            akaze_draw_stats = stat\n",
    "        orb.setMaxFeatures(stat.keypoints)\n",
    "        orb_res, stat = orb_tracker.process(frame)\n",
    "        orb_stats + stat\n",
    "        if (update_stats):\n",
    "            orb_draw_stats = stat\n",
    "        drawStatistics(akaze_res, akaze_draw_stats)\n",
    "        drawStatistics(orb_res, orb_draw_stats)\n",
    "        res_frame = cv2.vconcat([akaze_res, orb_res])\n",
    "        # cv2.imshow(video_name, akaze_res)\n",
    "        cv2.imshow(video_name, res_frame)\n",
    "        if (cv2.waitKey(1) == 27): # quit on ESC button\n",
    "           break\n",
    "\n",
    "    akaze_stats / (i - 1)\n",
    "    orb_stats / (i - 1)\n",
    "    printStatistics(\"AKAZE\", akaze_stats);\n",
    "    printStatistics(\"ORB\", orb_stats);\n",
    "    return 0\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display just the inlier matches. You should get something like this:\n",
    "\n",
    "<img src=\"img/lab06-2.png\" width=\"600\"/>\n",
    "\n",
    "in the original image pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undistorted video\n",
    "\n",
    "From the last lab, we have undistorted images in video and save it as YML file, please use it.\n",
    "\n",
    "Here is the camera parameters example.\n",
    "\n",
    "<code>\n",
    "%YAML:1.0\n",
    "---\n",
    "calibration_time: \"Tue 30 Jun 2020 06:30:37 AM +07\"\n",
    "image_width: 0\n",
    "image_height: 0\n",
    "board_width: 9\n",
    "board_height: 6\n",
    "square_size: 2.5000000000000001e-02\n",
    "camera_matrix: !!opencv-matrix\n",
    "   rows: 3\n",
    "   cols: 3\n",
    "   dt: d\n",
    "   data: [ 8.0705330599314675e+02, 0., 9.1415972512361793e+02, 0.,\n",
    "       8.0429430651971450e+02, 4.6984292440341721e+02, 0., 0., 1. ]\n",
    "distortion_coefficients: !!opencv-matrix\n",
    "   rows: 5\n",
    "   cols: 1\n",
    "   dt: d\n",
    "   data: [ -1.6058543270864040e-01, 2.7551377096678122e-02,\n",
    "       1.5304727307063063e-04, -7.0636905648298047e-05,\n",
    "       -2.4178142301529634e-03 ]\n",
    "rot: !!opencv-matrix\n",
    "   rows: 3\n",
    "   cols: 1\n",
    "   dt: d\n",
    "   data: [ 1.6390081888720540e+00, 3.1000248925614857e-02,\n",
    "       -4.3421312214750228e-02 ]\n",
    "trans: !!opencv-matrix\n",
    "   rows: 3\n",
    "   cols: 1\n",
    "   dt: d\n",
    "   data: [ 8.5053651529399948e-02, 3.8576576882373542e-01,\n",
    "       -3.4753891558701172e-01 ]\n",
    "rms-error: -1.\n",
    "</code>\n",
    "\n",
    "# Dr. Matt, pls help!\n",
    "About this, we do not have real rotation and translation in extrinsic parameters. Can we use the extrinsic parameters at above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the essential matrix\n",
    "\n",
    "Next, let's find an essential matrix relating these two images using the better keypoint matching algorithm from the previous experiment.\n",
    "\n",
    "\n",
    "## Undistort points\n",
    "\n",
    "You'll want to use undistortPoints to obtain \"ideal\" undistorted points for each of the input point sets.\n",
    "\n",
    "Be careful about the Mat object resulting from <code>undistortPoints()</code>. It is a Nx1 2 channel, 64-bit image, so to access it, you use code such as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Example of function\n",
    "Mat xy_undistorted;  // leave empty, opencv will fill it.\n",
    "undistortPoints(match_points, xy_undistorted, camera_matrix, dist_coeffs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Example of how to get the points\n",
    "Point2f point;\n",
    "for (int i = 0;i<nPoints;i++)\n",
    "{\n",
    "    point.x = xy_undistorted.at<cv::Vec2d>(i, 0)[0];\n",
    "    point.y = xy_undistorted.at<cv::Vec2d>(i, 0)[1];\n",
    "    // do something\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of function\n",
    "xy_undistorted = cv2.undistortPoints(match_points, camera_matrix, dist_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xy_undistorted[i][0]\n",
    "y = xy_undistorted[i][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing this in advance will save you some time.\n",
    "\n",
    "Next, use <code>findEssentialMat</code> to get an essential matrix.\n",
    "\n",
    "Pick two pairs of corresponding points in the two images and verify that $X^T K^{-T} E K^{-1} X' = 0$, approximately.\n",
    "\n",
    "Hint: you can tell <code>drawMatches</code> to only draw inliers by constructing a vector of vector of char like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std::vector<std::vector<char> > vvMatchesMask;\n",
    "for (int i = 0, j = 0; i < matched1.size(); i++) {\n",
    "    if (vMatched[i]) {\n",
    "        if (inlier_mask.at<uchar>(j)) {\n",
    "            vvMatchesMask.push_back( { 1, 0 } );\n",
    "        } else {\n",
    "            vvMatchesMask.push_back( { 0, 0 });\n",
    "        }\n",
    "        j++;\n",
    "    } else {\n",
    "        vvMatchesMask.push_back( { 0, 0 });\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchesMask = []\n",
    "j = 0\n",
    "for i in range(len(good)):\n",
    "    if (vMatched[i]) {\n",
    "        if (inlier_mask[j] > 0):\n",
    "            matchesMask.append( ( 1, 0 ) );\n",
    "        } else {\n",
    "            matchesMask.append( ( 0, 0 ) );\n",
    "        }\n",
    "        j++;\n",
    "    } else {\n",
    "        matchesMask.append( ( 0, 0 ));\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here <code>vMatched</code> is a vector of <code>bool</code> that I constructed while selecting matches according to the distance ratio.\n",
    "\n",
    "Using undistorted images and undistorted points (see note above about how to access the undistorted point array) you should get something like this:\n",
    "\n",
    "<img src=\"img/lab06-3.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, draw a couple corresponding epipolar lines in each undistorted image. You should get something like this:\n",
    "\n",
    "#### for frame 1\n",
    "\n",
    "<img src=\"img/lab06-4.png\" width=\"600\"/>\n",
    "\n",
    "#### For frame 2\n",
    "\n",
    "<img src=\"img/lab06-5.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your report, show your analysis of the number of keypoints, matched keypoints, matched unique keypoints (those that pass the distance ratio test), and inliers according to the estimated essential matrix.\n",
    "\n",
    "### Recover relative pose\n",
    "\n",
    "Use <code>correctMatches()</code> and <code>recoverPose()</cdoe> to \"clean up\" your image points (adjust each corresponding pair of points to be on corresponding epipolar lines according to E/F) and get the rotation and translation between the two camera frames. Understand the rotation and translation vectors you get and the scale ambiguity inherent in a metric 3D reconstruction.\n",
    "\n",
    "Construct the two projection matrices and use <code>triangulatePoints()</code> to obtain 3D points from the corrected 2D points. Visualize the 3D point cloud in Octave to see if it is sensible.\n",
    "\n",
    "You should get something similar to this:\n",
    "\n",
    "<img src=\"img/lab06-6.png\" width=\"600\"/>\n",
    "\n",
    "Here the points have been transformed from the first camera's coordinate frame to the robot frame for the first camera, using the rotation matrix and translation matrix from the extrinsic calibration.\n",
    "\n",
    "### Find absolute scale\n",
    "\n",
    "We know that after scaling then transforming the 3D points into the world coordinate system, the points with the smallest 'Z' values should be the ones on the floor. Can you come up with a scale factor that pushes the \"bottom\" of the point cloud to the floor (Z=0) in the world frame? Show your solution and a visualization of the points.\n",
    "\n",
    "After scaling the points in the camera frame (or re-triangulating after scaling the translation vector from <code>recoverPose()</code>), you should have a structure similar to what's shown in <link>[this video](https://drive.google.com/file/d/16lwooQ4rIGJJ1cLM-hUxb_m-tmmyWddY/view)</link>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
